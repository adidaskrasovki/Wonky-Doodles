{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cba24773",
   "metadata": {},
   "source": [
    "# Hello, Welcome!\n",
    "\n",
    "This Notebook uses Machine Learning / PyTorch for recognizing handwritten numbers, but can easily be modfied for other class recognition purposes. You may use this as a kickstarter or skeleton for your own purposes, or steal this entirely. Just call me out when you do so please.  \n",
    "\n",
    "This notebook is based on the Pytorch tutorial by Patrick Loeber:  \n",
    "https://www.youtube.com/watch?v=c36lUUr864M\n",
    "\n",
    "I'm using the Dataset from this link:  \n",
    "https://www.kaggle.com/datasets/jcprogjava/handwritten-digits-dataset-not-in-mnist\n",
    "\n",
    "### Known Issues:\n",
    "- Output flickers for Visual Studio. However, this seems to be an issue with VS and might be impossible to fix from my side.\n",
    "\n",
    "### If you are using Windows:  \n",
    "Copy/Paste the line below into the Windows cmd console if tensorboard won't start or is giving you a hard time in general.\n",
    "Kills the tensorboard process, which can be really tough and persists to run even after restart of your machine.\n",
    "\n",
    "    del /q %TMP%\\.tensorboard-info\\*\n",
    "\n",
    "Also, Microsoft Defender is slowing down Dataloading significantly.\n",
    "\n",
    "### If you are using Tensorboard:  \n",
    "Tensorboard creates new folders in your working directory. Make sure you have writing permissions or turn tensorboard off by setting\n",
    "\n",
    "    tb_analytics = False. \n",
    "\n",
    "Don't forget to delete everything in the folder once you're done here!\n",
    "\n",
    "### If you are using Jupyter Notebooks:  \n",
    "Use this link to start a tensorboard session in your browser:  \n",
    "http://localhost:6006/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29e1f0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### ALL THE IMPORTS ###\n",
    "#######################\n",
    "\n",
    "# PyTorch imports\n",
    "import torch as tc\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import torchvision as tv\n",
    "import torchvision.io as io\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Tensorboard imports\n",
    "import tensorboard\n",
    "from tensorboard import notebook\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "logdir =  './runs/'                 # dir in which to save run data\n",
    "writer = SummaryWriter(logdir)      # init tensorboard data writer\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=runs\n",
    "dir_counter = 0                     # counter for setting up tensorboard folders; different training runs will be saved in different folders\n",
    "\n",
    "# PIL Imports\n",
    "from PIL import Image\n",
    "from PIL import ImageStat\n",
    "from PIL import ImageOps\n",
    "from PIL import ImageShow\n",
    "from PIL import ImageFilter\n",
    "\n",
    "# Standard pkg imports\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from threading import Thread as Thread\n",
    "from threading import Event as Event\n",
    "\n",
    "clear_output()\n",
    "\n",
    "# CUDA\n",
    "if tc.cuda.is_available():\n",
    "    device = tc.device(\"cuda\")\n",
    "else:\n",
    "    device = tc.device(\"cpu\")\n",
    "\n",
    "print(f\"CUDA is available: {tc.cuda.is_available()}\")\n",
    "\n",
    "\n",
    "#### MODEL CLASSES ####\n",
    "#######################\n",
    "    \n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.lin1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.lin2 = nn.ReLU()\n",
    "        self.lin3 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.lin1(x)\n",
    "        out = self.lin2(out)\n",
    "        out = self.lin3(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "class ConvNN(nn.Module):\n",
    "\n",
    "    # Conv Output Size:\n",
    "    # OutputWidth = (Width - FilterSize + 2*Padding) / (Stride) + 1\n",
    "    def conv2d_out_dim(self, input_dim, kernel_size, padding, stride):\n",
    "        return ((tc.tensor(input_dim) - tc.tensor(kernel_size) + 2*tc.tensor(padding)) / (tc.tensor(stride))) + 1\n",
    "\n",
    "    def __init__(self, img_dim, fc1_dim, fc2_dim, output_dim):\n",
    "        super(ConvNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = img_dim[0],\n",
    "                               out_channels = 3,\n",
    "                               kernel_size = 5,\n",
    "                                )\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels = self.conv1.out_channels,\n",
    "                               out_channels = 16,\n",
    "                               kernel_size = 5,\n",
    "                                )\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size = 2,\n",
    "                                 stride = 2)\n",
    "        \n",
    "        self.adapter_dim = self.conv2d_out_dim(img_dim[1:len(img_dim)], self.conv1.kernel_size, self.conv1.padding, self.conv1.stride)\n",
    "        self.adapter_dim = self.conv2d_out_dim(self.adapter_dim, self.pool.kernel_size, self.pool.padding, self.pool.stride)\n",
    "        self.adapter_dim = self.conv2d_out_dim(self.adapter_dim, self.conv2.kernel_size, self.conv2.padding, self.conv2.stride)\n",
    "        self.adapter_dim = self.conv2d_out_dim(self.adapter_dim, self.pool.kernel_size, self.pool.padding, self.pool.stride)\n",
    "        self.adapter_dim = int((self.conv2.out_channels * self.adapter_dim[0] * self.adapter_dim[1]).item())\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features = self.adapter_dim, out_features = fc1_dim)\n",
    "        self.fc2 = nn.Linear(in_features = self.fc1.out_features, out_features = fc2_dim)\n",
    "        self.fc3 = nn.Linear(in_features = self.fc2.out_features, out_features = output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.pool(F.relu(self.conv1(x)))\n",
    "        out = self.pool(F.relu(self.conv2(out)))\n",
    "        out = out.view(-1, self.adapter_dim)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "###### FUNCTIONS ######\n",
    "#######################\n",
    "\n",
    "def loading_animation(event, message = 'loading'):                  # Thread, needs a halting event as input!\n",
    "    while(True):                                                    # prints a loading animation, run this code somewhere to see what it does.\n",
    "        print(message, sep='', end='')                              # google \"python threads\" if you are unfamiliar.\n",
    "        time.sleep(1)\n",
    "        if event.is_set():\n",
    "            clear_output()\n",
    "            break\n",
    "        for i in range(3):\n",
    "            print('.', sep='', end='')\n",
    "            time.sleep(1)\n",
    "            if event.is_set():\n",
    "                clear_output()\n",
    "                break\n",
    "        clear_output(wait = True)\n",
    "\n",
    "\n",
    "def get_batch(batches,                              # input dataset, batch-length must be > 0\n",
    "              batch_idx = 0,                        # get batch at batch_idx...\n",
    "              get_random = False):                  # ...or pick a random batch\n",
    "    \n",
    "    # Start loading screen\n",
    "    event = Event()\n",
    "    thread = Thread(target = loading_animation, daemon=True, args=(event, \"Loading Batches. This might take a while\"))\n",
    "    thread.start()\n",
    "\n",
    "    batches_iterator = iter(batches)                                            # init dataset iterator\n",
    "    if get_random:                                                              # set random index\n",
    "        batch_idx = random.randrange(len(batches))\n",
    "    else:                                                                       # for proper indexing\n",
    "        batch_idx += 1\n",
    "\n",
    "    for i in range(batch_idx):                                                  # iterate through dataset...\n",
    "        next(batches_iterator)\n",
    "        if i == batch_idx - 1:                                                  # ...until idx...\n",
    "            batch_spl, batch_lbl = next(batches_iterator)\n",
    "\n",
    "            # Stop loading screen\n",
    "            if not event.is_set():\n",
    "                event.set()\n",
    "                thread.join()\n",
    "            \n",
    "            return (batch_spl, batch_lbl)                                       # ...and return a tuple of form (batch of samples, batch of labels)\n",
    "\n",
    "\n",
    "def get_sample(batches,                             # input dataset, batch-length must be > 0\n",
    "               sample_idx = 0,                      # get sample at sample_idx,...\n",
    "               get_random = False):                 # ...or pick a random sample throughout all batches\n",
    "    \n",
    "    batch_idx, sample_idx = divmod(sample_idx, len(batches))\n",
    "    batch = get_batch(batches, batch_idx, get_random)\n",
    "    if get_random:\n",
    "        sample_idx = random.randrange(len(batch)) \n",
    "    return (batch[0][sample_idx], batch[1][sample_idx])                         # returns a tuple of form (sample, label)\n",
    "    \n",
    "\n",
    "def tb_write_model(model, batches):\n",
    "    writer.add_graph(model, get_sample(batches)[0].to(device))\n",
    "\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "def training_loop(model,                            # model input\n",
    "                  batches_trn,                      # training batches input\n",
    "                  criterion,                        # cost/loss/criterion function input\n",
    "                  optimizer,                        # ...\n",
    "                  scheduler,\n",
    "                  n_epochs = 1,                     # number of iterations through all batches\n",
    "                  tb_analytics = False,             # tensorboard plugin\n",
    "                  print_fps = 30.):                 # output fps. Needed for not overwhelming the kernel. Also serves to limit tensorboard datasize.\n",
    "    \n",
    "    # Start loading screen\n",
    "    event = Event()\n",
    "    thread = Thread(target = loading_animation, daemon=True, args=(event, \"Loading Batches. This might take a while\"))\n",
    "    thread.start()\n",
    "\n",
    "    # init func-global variables\n",
    "    n_batches = len(batches_trn)                                                    # number of batches\n",
    "    acc = 0.                                                                        # accuracy counter\n",
    "    n_iter = 0                                                                      # iter counter\n",
    "    t_0 = time.time()                                                               # save current time value\n",
    "    t_fps = time.time()                                                             # -- \" -- for output fps\n",
    "    \n",
    "    for epoch in range(n_epochs):                                                   # iter through epochs\n",
    "        for batch_idx, (inputs, labels) in enumerate(batches_trn):                  # iter through batches in epoch\n",
    "            \n",
    "            # Stop loading screen\n",
    "            if not event.is_set():\n",
    "                event.set()\n",
    "                thread.join()\n",
    "            \n",
    "            # Compute prediction and true value Block\n",
    "            output_prd = model(inputs.to(device))                                   # calc model output\n",
    "            output_tru = labels.to(device)                                          # get label\n",
    "            \n",
    "            # Running average accuracy Block\n",
    "            compare_results = 0\n",
    "            for outpt, label in zip(output_prd, output_tru):                        # calc number of correct predictions in batch\n",
    "                compare_results += outpt.argmax() == label.argmax()\n",
    "\n",
    "            batch_len = len(labels)                                                 # get size of current batch. Not necessarily equal to batch_size!\n",
    "            acc = (compare_results + n_iter * acc) / (n_iter + batch_len)           # calc running average accuracy\n",
    "            n_iter += batch_len                                                     # advance iteration counter\n",
    "\n",
    "            # Gradient Block\n",
    "            optimizer.zero_grad()                                                   # reset gradient calc\n",
    "            cost = criterion(output_prd, output_tru)                                # calc cost value\n",
    "            cost.backward()                                                         # backward propagation\n",
    "            optimizer.step()                                                        # apply optimizer\n",
    "            \n",
    "            # Output Block\n",
    "            if time.time() - t_fps >= 1./print_fps:\n",
    "                t_fps = time.time()\n",
    "                print(f'epoch {epoch + 1}/{n_epochs}; batch {batch_idx + 1}/{n_batches}; learning rate = {optimizer.param_groups[0][\"lr\"]}; Cost: {cost:.6f}; Running Accuracy: {100 * acc:.2f} %')\n",
    "                clear_output(wait = True)\n",
    "            # Define your tensorboard data here \n",
    "                if tb_analytics:\n",
    "                    writer.add_scalar('Training Loss', cost, batch_idx + epoch * n_batches)\n",
    "                    writer.add_scalar('Training Accuracy', acc, batch_idx + epoch * n_batches)\n",
    "            \n",
    "            # tidy up tensorboard writer\n",
    "            if tb_analytics:\n",
    "                writer.flush()\n",
    "                writer.close()\n",
    "            \n",
    "        scheduler.step()                                                            # diminish learning rate after every epoch\n",
    "    \n",
    "    t_1 = time.time()                                                               # get time value after training\n",
    "    print(f'Done. Final Cost: {cost:.6f}. Time: {(t_1 - t_0):.2f}s.')               # final output\n",
    "    return model                                                                    # returns the trained model\n",
    "    \n",
    "\n",
    "def validation_loop(model,                                                          # model input\n",
    "                    batches_tst,                                                    # test batches input\n",
    "                    print_miss = False,                                             # option for showing wrong predictions\n",
    "                    print_fps = 30.):                                               # output fps. Needed for not overwhelming the kernel\n",
    "    \n",
    "    with tc.no_grad():                                                              # don't train the model anymore!\n",
    "\n",
    "        # Start loading screen\n",
    "        event = Event()\n",
    "        thread = Thread(target = loading_animation, daemon=True, args=(event, \"Loading Batches. This might take a while\"))\n",
    "        thread.start()\n",
    "        \n",
    "        # init func-global variables\n",
    "        n_batches = len(batches_tst)                                                # number of batches\n",
    "        acc = 0.                                                                    # running accuracy counter\n",
    "        n_iter = 0                                                                  # iter counter\n",
    "        t_0 = time.time()                                                           # get current time value\n",
    "        t_fps = time.time()                                                         # -- \" -- for output fps\n",
    "    \n",
    "        for batch_idx, (inputs, labels) in enumerate(batches_tst):                  # iter through batches\n",
    "            \n",
    "            # Stop loading screen\n",
    "            if not event.is_set():\n",
    "                event.set()\n",
    "                thread.join()\n",
    "            \n",
    "            for inpt, label in zip(inputs, labels):                                 # iter through samples in batches\n",
    "                \n",
    "                output_prd = model(inpt.to(device))                                 # calc model output\n",
    "                output_tru = label.to(device)                                       # get label\n",
    "                compare_results = output_prd.argmax() == output_tru.argmax()\n",
    "                acc = (compare_results + n_iter * acc) / (1 + n_iter)               # running average accuracy\n",
    "                n_iter += 1\n",
    "\n",
    "                # Print miss Block\n",
    "                if print_miss and not compare_results:\n",
    "                    print(f'batch {batch_idx + 1}/{n_batches}; Accuracy: {100*acc:.2f} %')\n",
    "                    print(f'Miss at Iteration {n_iter}! Predicted: {output_prd.argmax().item()} (Confidence = {100 * tc.softmax(output_prd, dim=0).max().item():.2f} %), True: {output_tru.argmax().item()}')\n",
    "                    plt.imshow(inpt.reshape(28,28), cmap='gray')\n",
    "                    plt.axis('off')\n",
    "                    plt.show()\n",
    "                    time.sleep(2)\n",
    "                    clear_output(wait = True)  \n",
    "                # Output Block\n",
    "                if time.time() - t_fps >= 1./print_fps:\n",
    "                    t_fps = time.time()\n",
    "                    print(f'batch {batch_idx + 1}/{n_batches}; Accuracy: {100*acc:.2f} %')\n",
    "                    clear_output(wait = True)\n",
    "\n",
    "                    \n",
    "        print(f'Done. Final Accuracy: {100*acc:.2f} %. Time: {(time.time() - t_0):.2f}s.')  # final output\n",
    "    return acc                                                                      # returns the final accuracy\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fac966b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PREPARE DATA ####\n",
    "###### - 0.1 - #######\n",
    "\n",
    "\n",
    "# Adapter from your Dataset to Dataloader and thus specific for each Dataset.\n",
    "# Microsoft Defender might slow things down here significantly. Take a look at your task manager.\n",
    "\n",
    "# init func-global variables\n",
    "datapath = './dataset/'\n",
    "n_samples = 10 * 10772                                       # define total number of samples\n",
    "train_test_ratio = 0.75                                      # define ratio of train/total samples\n",
    "\n",
    "# Start loading screen\n",
    "event = Event()\n",
    "thread = Thread(target = loading_animation, daemon=True, args=(event, \"Loading Data into Memory. This might take a few minutes\"))\n",
    "thread.start()\n",
    "\n",
    "# create a tuple-list with entries of form (\"Digit\", Digit_sample), e.g. (\"1\", 9001)...\n",
    "address_list = []\n",
    "for i in range(10):\n",
    "    for j in range(10772):\n",
    "        address_list.append((i,j))\n",
    "\n",
    "np.random.shuffle(address_list)                             # ...and shuffle it.\n",
    "\n",
    "# loop through samples and labels\n",
    "# and load them into memory as tuples (samples, labels).\n",
    "digit = tc.zeros(10, dtype = tc.float32)                    # create a 10-dim-vector of zeros...\n",
    "for idx, (label, sample) in enumerate(address_list):\n",
    "    digit[label] = 1.                                       # ...and change a zero to a one in the spot representing the current sample digit...\n",
    "\n",
    "    # Load the image as flattened matrix (=vector) or matrix, respectively\n",
    "    # sample_cpy = (tc.flatten(tv.transforms.ToTensor()(Image.open(f\"./dataset/{label}/{label}{sample}.png\").getchannel('A'))), digit.clone())\n",
    "    sample_cpy = (tv.transforms.ToTensor()(Image.open(f\"{datapath}{label}/{label}{sample}.png\").getchannel('A')), digit.clone())\n",
    "    \n",
    "    address_list[idx] = sample_cpy\n",
    "    digit[label] = 0.                                       # ...and reset it afterwards.\n",
    "\n",
    "\n",
    "# Split the address list into a training and a test list and delete the original list afterwards\n",
    "trn_list = address_list[0 : math.floor(len(address_list) * train_test_ratio)]\n",
    "tst_list = address_list[math.ceil(len(address_list) * train_test_ratio) : len(address_list)]\n",
    "del address_list\n",
    "\n",
    "# Stop loading screen\n",
    "event.set()\n",
    "thread.join()\n",
    "\n",
    "# final output\n",
    "clear_output(wait = True)\n",
    "print(f'Length of Training List: {len(trn_list)} items.')\n",
    "print(f'Length of Test List: {len(tst_list)} items.')\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c37846c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### DATALOADER #####\n",
    "###### - 0.2 - #######\n",
    "\n",
    "\n",
    "# Shove both training and test lists into the dataloader.\n",
    "\n",
    "batch_size = 4                                                  # define batch_size\n",
    "\n",
    "batches_trn = DataLoader(dataset = trn_list,                    # samples input\n",
    "                     batch_size = batch_size,\n",
    "                     shuffle = False,                           # no need to shuffle the samples, we already did that.\n",
    "                     num_workers = 0,                           # strongly recommended to keep = 0\n",
    "                     persistent_workers = False,\n",
    "                     pin_memory = True)                         # prevent loading into memory after every epoch\n",
    "\n",
    "batches_tst = DataLoader(dataset = tst_list,\n",
    "                     batch_size = batch_size,\n",
    "                     shuffle = False,\n",
    "                     num_workers = 0,\n",
    "                     persistent_workers = False,\n",
    "                     pin_memory = True)\n",
    "\n",
    "# tidy up\n",
    "del trn_list\n",
    "del tst_list\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569b82f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### DEFINE MODEL ####\n",
    "######## - 1 - ########\n",
    "\n",
    "model = ConvNN(tc.tensor(get_sample(batches_trn)[0].shape),\n",
    "               fc1_dim = 64,\n",
    "               fc2_dim = 64,\n",
    "               output_dim = 10\n",
    "               ).to(device)\n",
    "\n",
    "# Optional Block for loading in a model and/or model-state from disk\n",
    "# filepath = './'\n",
    "# filename = \"Tutorial_CNN.pth\"\n",
    "# model = tc.load(f\"{filepath}{filename}\").to(device)\n",
    "# model.load_state_dict(tc.load(f\"{filepath}{filename}_state_dict\"))\n",
    "\n",
    "###### LOSS/COST ######\n",
    "###### OPTIMIZER ######\n",
    "###### SCHEDULER ######\n",
    "######## - 2 - ########\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = tc.optim.Adam(model.parameters(),\n",
    "                         lr = .001)                                     # define initial learning rate\n",
    "\n",
    "step_lr_scheduler = lr_scheduler.StepLR(optimizer,\n",
    "                                        step_size = 1,                  # diminish learning rate every n-th epoch...\n",
    "                                        gamma = .1)                     # ...by this diminishing factor\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa328f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### TRAINING #######\n",
    "######## - 3 - ########\n",
    "\n",
    "\n",
    "# Optional Block for using tensorboard\n",
    "tb_analytics = True\n",
    "\n",
    "\n",
    "if tb_analytics:\n",
    "    logdir = f\"./runs/{dir_counter}/\"\n",
    "    writer = SummaryWriter(logdir)\n",
    "    tb_write_model(model, batches_trn)\n",
    "    dir_counter += 1\n",
    "\n",
    "\n",
    "# Training Loop. \n",
    "# Note that loading the batches into memory might take a few minutes. Take a look at your task manager.\n",
    "\n",
    "model_trn = training_loop(n_epochs = 2,                             # See func definition for input details\n",
    "                          print_fps = 5.,\n",
    "                          model = model.to(device),\n",
    "                          batches_trn = batches_trn,\n",
    "                          criterion = criterion.to(device),\n",
    "                          optimizer = optimizer,\n",
    "                          scheduler = step_lr_scheduler,\n",
    "                          tb_analytics = tb_analytics)                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd2597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### TESTING #######\n",
    "######## - 4 - ########\n",
    "\n",
    "\n",
    "# Testing Loop.\n",
    "# Again, loading batches into memory may take a few minutes. Stay strong.\n",
    "\n",
    "device = tc.device(\"cpu\")                                           # optional, but runs faster on cpu for some reason\n",
    "accuracy = validation_loop(print_fps = 5.,                          # See func definition for input details\n",
    "                           print_miss = True,\n",
    "                           model = model_trn.to(device),\n",
    "                           batches_tst = batches_tst)\n",
    "\n",
    "if tc.cuda.is_available():                                          # change back to gpu\n",
    "    device = tc.device(\"cuda\")\n",
    "else:\n",
    "    device = tc.device(\"cpu\")\n",
    "model_trn = model_trn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bc1d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### SAVE MODEL ######\n",
    "######## - 5 - ########\n",
    "\n",
    "# Set filepath and model name\n",
    "filepath = './'\n",
    "model_name = 'Tutorial_CNN'\n",
    "\n",
    "tc.save(model_trn, f\"{filepath}{model_name}.pth\")                               # Save the whole model and/or...\n",
    "tc.save(model_trn.state_dict(), f\"{filepath}{model_name}_state_dict.pth\")       # ...save only the the model state\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bee210",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# I'm just here so that the Screen doesn't flick when printing images\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
